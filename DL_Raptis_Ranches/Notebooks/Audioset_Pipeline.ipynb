{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audioset\n",
    "After dowloading the data set this jupyter will provide all the nessecary steps needed to utilize it annd train a model with it\n",
    "We highly recomend to download the dataset in a cloud because some of the files are downloaded in their full size and we cut them later.\n",
    "Whetever we provide in dowload section should be skipped if the dataset is already download and cutted into 10 sec windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowload process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Essential Procerdures for the dowload to work\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "directory = 'C:\\\\Users\\\\giorg\\\\OneDrive\\\\Υπολογιστής\\\\DL Project'\n",
    "os.chdir(directory)\n",
    "\n",
    "from Scripts import Utilities as ut\n",
    "from Scripts import Feature_Extraction as fe\n",
    "from Scripts import Model_Training as mt\n",
    "from Scripts import data_loading as dl\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "dataset = \"audioset\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the audioset repository\n",
    "!git clone https://github.com/lukefahr/audioset.git\n",
    "%cd audiosetAC\n",
    "!python3 -m pip install --force-reinstall https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts import Audioset_Parser as ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cloning the directory o folder which is called metadat will be finoun inside it the are the \"balanced_train_segments \"and \"eval_segments\" csv need to dowload the files and cut them from Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.download_and_cut_audioset(\"segments_csv\", \"audioset_dowload_directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After download you need to flagg files which where created before the date you started dowloading the dataset , this is the way we found to discriminate which files where correctly cuted and what where not. Then you move these files in another directory and from them you cut them based on the csv again localy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "All paths are example paths change them with oyur own this is the pipeline of the procedure we follow and you oly need to dowload the Audioset Train and Eval\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cutoff_date = datetime(2024, 6, 6).timestamp() # Example date\n",
    "folder_path = \"audioset_dowload_directory\"\n",
    "\n",
    "ut.flag_old_files(folder_path, cutoff_date)\n",
    "\n",
    "ut.move_files_with_prefix(folder_path,\"flaged_files_dest\", \"FLAG-\")\n",
    "\n",
    "ut.delete_files_with_prefix(\"flaged_files_dest\",\"FLAG-\")\n",
    "\n",
    "ap.cut_audio( \"train_segments_csv\",\"flaged_files_dest\", \"AS_Train\") # Change for eval\\\n",
    "\n",
    "ap.convert_audio(folder_path, \"AS_Train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Labels\n",
    "\n",
    "After you downloaded AS_Train and AS_Eval you need to extract the labels for the csv which is located at the ontology.json which is in git hub directory we mounted at the start \"balanced_train_segments.csv\" and \"eval_Segments.csv\" is also there but we will provide them loccally ath the \"Audioset\\\\CSVs\" destination.\n",
    "\n",
    "------ Before starting got to both csv and add as columns class 1,class 2,class 3,.......,class_12,class 13----------------\n",
    "We have done it here but if you want to replicate from scratch you need to do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"Audioset\\\\CSVs\\\\balanced_train_segments.csv\"\n",
    "ontology_json = \"Audioset\\\\CSVs\\\\ontology.json\"\n",
    "output_csv= \"Audioset\\\\CSVs\\\\balanced_train_labels.csv\"\n",
    "sound_path = \"Audioset\\\\AS_Train\"\n",
    "\n",
    "ap.create_labels(input_csv, ontology_json, output_csv)\n",
    "# Because we created the function to work with csv we put the same output p[ath as input path so it will overwrite the file\n",
    "ap.labels_to_use(sound_path, output_csv, output_csv)\n",
    "# Seperate files into class fodler\n",
    "ut.move_to_class_folder(\"Audioset\\CSVs\\GPTTrain.csv\",\"Audioset\\AS_Train\",dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"Audioset\\CSVs\\eval_segments.csv\"\n",
    "ontology_json = \"Audioset\\\\CSVs\\\\ontology.json\"\n",
    "output_csv= \"Audioset\\\\CSVs\\\\eval_labels.csv\"\n",
    "sound_path = \"Audioset\\\\AS_Eval\"\n",
    "\n",
    "ap.create_labels(input_csv, ontology_json, output_csv)\n",
    "ap.labels_to_use(sound_path, output_csv, output_csv)\n",
    "\n",
    "ut.move_to_class_folder(\"Audioset\\CSVs\\GPTEval.csv\",\"Audioset\\AS_Eval\",dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the csv with the files what we tried to do is to aplly NLP methods and clustering to cluster files that had similar names to 8 categories whatever we did was nothing compared to what GPT-4o gave us , to put it in prespective from 20% accuracy it skyrocketed to 50% so it would a shame not to use it, nevertheless we include this in the DL_Project in Bonus folder because we think it was a quite hinest try. We will go on with the gpt labels which are located at \"DL_Project\\Audioset\\CSVs\"\n",
    "\n",
    "\n",
    "- We asked GPT to seperate our folders into 10 categories but some of them where oversampled and 1 very undersabled (140 files total) to put it prespective other folders had form 2.000 to 8.000 files so we did not include this category in training. We made othe manual modifications but we will dig into that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "The same method as we did in the UrbanSound8k is used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION Name feature folders Train_Features, Eval_Features corespondingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.CNN_Features_mel(\"Audioset\\AS_Train\", classes, \"Audioset\\\\Train_Features\")\n",
    "fe.CNN_Features_mel(\"Audioset\\AS_Eval\", classes, \"Audioset\\\\Eval_Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val =  dl.load_data(\"Audioset\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mt.create_model_CNN_AS()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate = learning_rate ), loss='categorical_crossentropy', metrics=[mt.f1_score, 'accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_f1_score', mode='max', patience=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=50, callbacks = early_stopping,\n",
    "    batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = hist.history['f1_score']\n",
    "c = hist.history['val_f1_score']\n",
    "mt.plot_training_history(_,c,\"f1_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.plot_confusion_matrix(model,x_val,y_val,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models\\\\model_name.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
